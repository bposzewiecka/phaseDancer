import os
from src.utils.text_utils import get_prev_number, get_range
from src.scripts.read_clustering_and_extraction import cluster_and_extract_reads
from src.scripts.reference_extension import extend_contig
from src.scripts.merge_contigs import merge_contigs

from src.utils.yaml_utils import load_yaml
from src.config.configuration import snakemake_validate_config, TECHNOLOGIES_CONFIG_FN, CONFIG_FN, get_number_of_indices

def recursive_input(input_file):

    def func(wildcards):

        if wildcards.number == '000':
            number = '000'
        else:
            number = get_prev_number(wildcards.number)

        return input_file.format(**wildcards).format(number = number)

    return func

PHASEDANCER_DATA_DIR = os.environ['PHASEDANCER_DATA_DIR']
OUTPUT_DIR = PHASEDANCER_DATA_DIR + '/'

configfile: OUTPUT_DIR + CONFIG_FN

config_technologies = load_yaml(TECHNOLOGIES_CONFIG_FN)

errors = snakemake_validate_config(CONFIG_FN, check_files = False)

ruleorder:
    copy_first_fasta > clusterng_cluster_selection_read_extraction

ruleorder:
    copy_first_fasta > reference_extention


def get_config_value(key, sample, contig = None):

    value = None

    if key in config['samples'][sample]:
        value = config['samples'][sample][key]

    if contig is not None and type(config['samples'][sample]['contigs']) == dict and key in config['samples'][sample]['contigs'][contig]:
        value = config['samples'][sample]['contigs'][contig][key]

    return value

def get_input_dicts():

    def get_params(sample,  contig):

        return { 'assembler': 'minimap2', 'cluster': '000', 'start_number': '000', 'sample': sample, 'contig': contig, 'end_number':  get_config_value('iterations', sample, contig), 'cl_type': 'nc'}

    def get_sample_params(sample):

        contigs = get_config_value('contigs', sample)

        if type(contigs) == dict:

            contigs = contigs.keys()

        return [ get_params(sample, contig) for contig in contigs]

    if config['sample'] == 'all':

        data = []

        for sample in config['samples']:
            data += get_sample_params(sample)

        return data

    elif config['contig'] == 'all':

         return get_sample_params(config['sample'])

    else:

        return [ get_params(config['sample'],  config['contig'])]

rule main:
    input:
        [ expand(OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/merged_contigs/seq_{start_number}.seq_{end_number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.merged_contig.fasta', **input_dict)  for input_dict  in get_input_dicts()]


#
# Copying the first anchor sequence to the convenient place in the workflow directory structure.
# Fasta is converted to 2-line format.
#

rule copy_first_fasta:
    input:
        fasta = OUTPUT_DIR + 'data/{sample}/{contig}/{contig}.fasta'
    output:
        fasta = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/nc_{cluster}/seq_000/seq_000.{contig}.nc_{cluster}.{sample}.{assembler}.uncompressed.fasta'
    params:
        read_name = 'seq_000_{contig}_nc_{cluster}_{sample}_{assembler}'
    shell:
        "(echo '>{params.read_name}' ; tail -n +2  {input.fasta} | tr -d '\\n' ; echo '') > {output.fasta}"


#
# Generating files with few reads from the sample.
# Reads are used during the mapping procedure for detecting the end of the output generated by the anchor sequence.
#

rule generate_dummy_file:
    input:
        ref = OUTPUT_DIR + 'data/{sample}/index/{sample}_0.fasta'
    output:
        OUTPUT_DIR + 'data/{sample}/{assembler}/{cl_type}_{cluster}/{contig}.{cl_type}_{cluster}.{sample}.{assembler}.dummy.fasta'
    params:
        read_name_pattern = '>dummy_%03d_{contig}_{cl_type}_{cluster}_{sample}_{assembler}'
    shell:
        """
        grep -A 1 --max-count=5 '>' {input.ref} |
        awk '{{ if (NR%2==1) {{ printf("{params.read_name_pattern}\\n",(NR-1)/2) }} else {{ print($0) }} }} ' > {output}
        """


#
# Searching for reads similar to the anchor sequence by mapping the anchor sequence on minimap2 index preloaded into the RAM memory.
#
# Fasta file with an anchor sequence and some randomly selected reads are sent to the fifo queue (the fifo queue in turn sends it to the standard input of the mapper).
# This is done using the "flock" command to avoid the race conditions if many such commands are executed concurrently.
# Next, reads appended to the output fasta file (by process that receive output of the mapper) are read by "tail" command with -f parameter to output reads when the file grows.
# The outputed reads are read by "grep" command until a read with name 'END' is outputted by the mapper.
#

rule map_with_index_in_memory:
    input:
        index = OUTPUT_DIR + 'data/{sample}/index/{sample}_{index_no}.mmi',
        dummy = OUTPUT_DIR + 'data/{sample}/{assembler}/{cl_type}_{cluster}/{contig}.{cl_type}_{cluster}.{sample}.{assembler}.dummy.fasta',
        fasta = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.uncompressed.fasta'
    output:
        fasta = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.{index_no}.from_index.fasta'
    params:
        end_text = 'END',
        fifoin = OUTPUT_DIR + 'data/{sample}/index/{sample}_{index_no}_in.fifo',
    shell:
        'touch {output.fasta}; truncate -s 0 {output.fasta}; flock {input.index} --command "cat {input.dummy} {input.fasta} {input.dummy} > {params.fifoin}";  grep -A1 -m 1 {params.end_text} <(tail -f {output.fasta})'


#
# Homopolymer compression of fasta files.
#

rule compress_fasta:
    input:
        '{name}.uncompressed.fasta'
    output:
        '{name}.compressed.fasta'
    shell:
        'cat {input} | python compress_homopolymers.py compressed > {output} '


#
#  Mapping of homopolymer-compressed reads on the homopolymer-compressed  anchor sequence.
#
#  Reads are homopolymer-compressed and mapped on the homopolymer-compressed anchor sequence. Then they are filtered using "samtools view" command to left only primary mappings.
#  Then output in sam format is filtered using "gawk" command to left only mappings that have alignment length greater than "minlen" parameter.
#

rule map_reads_minimap:
    input:
        ref = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.minimap2.{compressed}.fasta',
        reads = lambda wildcards: [ OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.minimap2.{index_no}.from_index.fasta'.format(**wildcards, index_no=index_no) for index_no in range(get_number_of_indices(wildcards, config))],
        fai = OUTPUT_DIR + 'data/{sample}/{sample}.fasta.fai.done'
    output:
        bam = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.minimap2.{compressed}.bam'
    threads:
        1
    params:
        preset = lambda wildcards: config_technologies['technology'][config['samples'][wildcards['sample']]['technology']]['minimap2-preset'],
        minlen = 1000
    log:
        minimap = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/logs/minimap2/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.minimap2.{compressed}.log'
    shell:
       """
        cat  {input.reads} | python compress_homopolymers.py {wildcards.compressed} | minimap2 -t {threads} -ax {params.preset} --secondary=no {input.ref} - 2> {log.minimap} |
        samtools view -h -@ {threads} -F 2048 |
        gawk '{{
            match($6, /^([0-9]+)(.)/, start_arr);
            match($6, /([0-9]+)(.)$/, end_arr);
            read_len = length($10);
            if (start_arr[2] == "S") read_len -= start_arr[1];
            if (end_arr[2] == "S") read_len -= end_arr[1];
            if (substr($1, 1, 1) == "@" || read_len >= {params.minlen}) print($0)
         }}' |
        samtools sort -@ {threads} > {output}
	"""

#
# Indexing bam file
#

rule index_bam:
    input:
        '{file_name}.bam'
    output:
        '{file_name}.bam.bai'
    shell:
        'samtools index {input}'

#
# Clustering, cluster selection and read extraction.
# Implementation details can be found in the description of function "cluster_and_extract_reads" in module "src.scripts.clustering"..
#

rule clusterng_cluster_selection_read_extraction:
    input:
        compressed_bam = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.compressed.bam',
        compressed_bai = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.compressed.bam.bai',
        uncompressed_bam = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.uncompressed.bam',
        uncompressed_bai = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.uncompressed.bam.bai',
        compressed_fasta = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.compressed.fasta',
	uncompressed_fasta = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.uncompressed.fasta'
    output:
        reads = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.for_contig_extension.fasta',
        clusters =  OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/clusters/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.clusters.tsv',
        selected_cluster = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/clusters/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.selected_cluster.yaml',
	compressed_colored_bam =  OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/clusters/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.clusters.compressed.bam',
        compressed_colored_bai =  OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/clusters/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.clusters.compressed.bam.bai',
        uncompressed_colored_bam =  OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/clusters/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.clusters.uncompressed.bam',
        uncompressed_colored_bai =  OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/clusters/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.clusters.uncompressed.bam.bai',
	extension_size = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/clusters/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.extension_size.yaml',
    params:
        prev_selected_cluster = recursive_input(OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/clusters/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.selected_cluster.yaml'),
        prev_selected_cluster2 = recursive_input2(OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/clusters/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.selected_cluster.yaml'),
	prev_colored_bam = recursive_input(OUTPUT_DIR +  'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/clusters/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.clusters.bam'),
        prev_clusters = recursive_input(OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/clusters/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.clusters.tsv'),
        prev_clusters2 = recursive_input2(OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/clusters/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.clusters.tsv')
    run:
        try:
            cluster_and_extract_reads(int(wildcards.number), {**input, **output, **params})
        except Exception as e:
            print('Error in clustering and read extraction:', e)
            raise e


#
# Running wtdbg2 assembler on selected reads.
# The assembly is copied into convenient place in the workflow directory structure.
#

rule assembly_wtdbg2:
    input:
        reads = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.for_contig_extension.fasta'
    output:
        reads = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.assembled_contig.fasta'
    params:
        output_name = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/wtdbg2/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.for_contig_extension',
	wtdbg2_dir = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/wtdbg2',
        preset = lambda wildcards: config_technologies['technology'][config['samples'][wildcards['sample']]['technology']]['wtdbg2-preset'],
        wtdbg2_contigs = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/wtdbg2/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.for_contig_extension.raw.fa'
    log:
        err = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/logs/wtdbg2/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.for_contig_extension.log.err',
        out = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/logs/wtdbg2/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.for_contig_extension.log.out'
    shell:
        'mkdir -p {params.wtdbg2_dir}; wtdbg2.pl -o {params.output_name} -g 21000 -x {params.preset} {input} > {log.out} 2> {log.err} ; cp {params.wtdbg2_contigs} {output}'


#
# Reference extension of the anchor sequence. During this process a new anchor sequence is created for the next iteration of the workflow.
# Implementation details can be found in ..
#

rule reference_extention:
    input:
        assembled_contig = recursive_input(OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.assembled_contig.fasta')
    output:
        extended_contig = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.uncompressed.fasta',
        extension_info = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{number}/seq_{number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.extension_info.yaml'
    params:
        contig = recursive_input(OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.uncompressed.fasta'),
        extension_size = recursive_input(OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/clusters/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.extension_size.yaml'),
    run:
        contig_extension_size = min(get_config_value('contig-extension-size', wildcards.sample, wildcards.contig), load_yaml(params.extension_size)['extension_size'])
        extend_contig(params.contig, input.assembled_contig, output.extended_contig, output.extension_info, contig_extension_size)

fasta_contig_pattern = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/seq_{{number}}/seq_{{number}}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.uncompressed.fasta'

#
# Merging contigs generated during iteratve assembly of contig.
# Implementation details can be found in ..
#

rule merge_contigs:
    input:
        reads = lambda wildcards: expand(fasta_contig_pattern.format(**wildcards), number = get_range(start = wildcards['start_number'], end = wildcards['end_number']) )
    output:
        fasta = OUTPUT_DIR + 'data/{sample}/{contig}/{assembler}/{cl_type}_{cluster}/merged_contigs/seq_{start_number}.seq_{end_number}.{contig}.{cl_type}_{cluster}.{sample}.{assembler}.merged_contig.fasta'
    run:
        merge_contigs(output.fasta, wildcards, OUTPUT_DIR)
